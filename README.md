

Powered by Llama 3.1 & Flask




Sobre el Proyecto

Arquitectura

Caracter√≠sticas

Instalaci√≥n y Uso

Estructura del Proyecto

Equipo de Desarrollo

üìñ Sobre el Proyecto

Este proyecto, desarrollado como parte del curso de Inteligencia Artificial en la Universidad Andina del Cusco, busca solucionar la frustraci√≥n com√∫n de los desarrolladores: el tiempo perdido en errores de sintaxis y l√≥gica simple.

Utilizando la potencia de los Large Language Models (LLM), espec√≠ficamente Meta Llama 3.1-8B, nuestra aplicaci√≥n ofrece un entorno tipo IDE web donde los usuarios pueden pegar su c√≥digo y recibir correcciones instant√°neas o sugerencias de optimizaci√≥n, manteniendo el estilo y la estructura original.

üèó Arquitectura

El sistema sigue una arquitectura cliente-servidor desacoplada que permite la inferencia en hardware local o servidores remotos.

graph TD
    User[üë§ Usuario Dev] -->|Input C√≥digo| Frontend[üíª Web Interface (HTML/JS)]
    Frontend -->|POST Request| Backend[üêç Flask Server]
    Backend -->|Prompt Engineering| LLM_API[ü§ñ Llama 3.1 API (Local/Cloud)]
    LLM_API -->|C√≥digo Corregido| Backend
    Backend -->|JSON + Highlight| Frontend
    Frontend -->|Display| User


Stack Tecnol√≥gico

Frontend: HTML5, CSS3 (VS Code Dark Theme), Vanilla JS, Highlight.js.

Backend: Python, Flask.

IA Engine: Meta Llama 3.1-8B-Instruct (v√≠a API local/remota).

Comunicaci√≥n: REST API.

‚ú® Caracter√≠sticas

üé® Interfaz IDE-Style: Dise√±o oscuro inspirado en VS Code para reducir la fatiga visual.

‚ö° Correcci√≥n Inteligente: Detecta errores de sintaxis y l√≥gica en m√∫ltiples lenguajes (Python, JS, Java, C++, etc.).

üîÑ Refactorizaci√≥n (Python): Bot√≥n exclusivo para optimizar c√≥digo Python, mejorando eficiencia y legibilidad sin alterar la funcionalidad.

üìù Explicaci√≥n de Cambios: No solo arregla el c√≥digo, sino que explica qu√© cambi√≥ y por qu√© en una secci√≥n de notas.

üåà Syntax Highlighting: Resaltado de sintaxis autom√°tico para f√°cil lectura.

üöÄ Instalaci√≥n y Uso

Sigue estos pasos para desplegar el aplicativo en tu entorno local.

Prerrequisitos

Python 3.8 o superior.

Acceso a un servidor de inferencia Llama 3 (LM Studio, Ollama, o API Local).

Pasos

Clonar el repositorio

git clone [https://github.com/tu-usuario/ai-code-fixer.git](https://github.com/tu-usuario/ai-code-fixer.git)
cd ai-code-fixer


Instalar dependencias

pip install -r requirements.txt


Configurar el Modelo
Abre app.py y aseg√∫rate de que la variable API_URL apunte a tu servidor de inferencia.

# En app.py
API_URL = "http://localhost:1234/v1/chat/completions" # Ejemplo con LM Studio


Ejecutar la aplicaci√≥n

python app.py


Acceder
Abre tu navegador y ve a http://127.0.0.1:5000.





<div align="center">





<p>Universidad Andina del Cusco - 2025-II</p>
<img src="https://www.google.com/search?q=https://upload.wikimedia.org/wikipedia/commons/f/fa/Escudo_de_la_Universidad_Andina_del_Cusco.png" width="50" alt="Logo UAC">
</div>
